With the introduction of NUMA architecture, few problems with respect to data locality became bottleneck for a process's performance.
There is always a tradeoff between cache contention and interconnect overhead while scheduling threads on NUMA machines \cite{Majo:2011:MMN:1993478.1993481}
Running two threads of a process on a same socket avoids interconnect overhead but experiences a penalty in terms of cache contention, as last level cache will be shared between two threads running on the same socket.
Whereas scheduling two threads on different sockets can avoid cache contention as two threads will use their local LLCs but the penalty of data fetch over interconnect hits the performance in this case.
Majo et. al. \cite{Majo:2011:MMN:1993478.1993481} have introduced N-MASS algorithm, which is a cache conscious scheduler for NUMAs.


Problem discussed above talks about the data section associated with each process.
NUMA architecture introduces a similar problem in case of code section of each process as well.
Code section consists of instructions, and this section is read-only, unlike the data section.
Hence, there is always a single copy of this section in the memory which is shared between all the threads spawned by the process.
Code section can be considered small enough to be cached in the L1-icache of each CPU, which is true, and might not harm the performance as badly as in case of data.
But the code section can increase drastically once the thread starts using shared libraries included in the code.
Shared libraries are loaded at the runtime when the first thread linked against it is executed.
Once the shared library is loaded, its position does not change in the memory and only single copy exist in memory.
Size of these shared libraries can easily exceed the size of L1-icache (L1-icache in a AMD opteron machine is 64KB and size of libc is xxx).
Therefore, the instructions are always fetched from memory at every L1-icache miss.
Fetching these instructions over the interconnect harms the execution time of threads on remote sockets.
We observed that the threads running on the socket local to the shared library location takes least number of cycles to finish the process.
Whereas the socket farthest to it takes the maximum time.
In our four socket AMD Opteron machine, two sockets are directly connected to the local socket and the diagonaly opposite socket is accessible via single hop.
The number of cycles taken by the threads running on each of the sockets were proportional to the distance with the local socket.
The other factor which affects the thread performance on NUMA is the interconnect congestion.
As there is only single copy of shared library in memory, with the increasing number of threads the traffic across interconnect increases.
All remote threads experience a race condition for interconnect usage.
Due to which the number of stalled cycles increases drastically with increasing number of threads.


In this paper we are presenting the analysis about performance hit seen by processes due to code section, on NUMA machines.

