
With the constantly increasing number of cores in modern multicore systems, Non-uniform memory architectures
are becoming more and more common. In its simplest form, a NUMA can have two processors with local memories connected
to each other via an interconnect. Naturally, access to local memory is faster than the remote by a factor greater than 1,
called the numa factor or numa overhead. With more processors, the interconnections become more complex, and the numa factor
can also be different for different pair of nodes.

With the introduction of NUMA architectures, problems with respect to data locality becoming bottleneck for application performance
has drawn significant academic attention. Previous work by Brecht\cite{Brecht:1993:IPA:1295480.1295481} and 
Broquedis et al.\cite{numaScheduling} explore placement decisions in context of data locality.
In another recent work \cite{Dashti:2013:TMH:2490301.2451157}, the authors claim that contrary to older systems,
modern NUMA hardware has much smaller remote wire delays, and so remote access costs per se are not the main 
concern for performance, instead, congestion on memory controllers and interconnects, caused by memory traffic 
from data-intensive applications, hurts performance a lot more.

Another work by Majo et. al. \cite{Majo:2011:MMN:1993478.1993481} tends to exploit the trade-off between cache contention
and interconnect overhead while scheduling threads on NUMA machines. Running two threads of a process on the same socket
avoids interconnect overhead but experiences a penalty in terms of cache contention, as last level cache are shared for
threads running on the same socket, Whereas scheduling two threads on different sockets can avoid cache contention
as two threads will use their local LLCs but the penalty of data fetch over interconnect can hit the performance in this case.
Majo et. al. \cite{Majo:2011:MMN:1993478.1993481} have introduced N-MASS algorithm, which is a cache conscious scheduler for NUMAs.

The problems addressed in previous works mainly deal with the data-section associated with the program. We believe that
a similar problem exists for text-section of the program in context of NUMA environment, albeit less aggravated
because of the text-section being read-only. In this work, we present a detailed analysis for this problem in context of
shared libraries, which by definition has a single copy across all memory nodes, and is loaded into memory when an object
linked against it starts execution.

Text-section of a program mostly consists of instructions and read-only global data. In case of shared libraries, this section
is unique across all memory nodes and will be shared by all the running processes linked against that library. As the text
section is read-only and would be rarely evicted, it makes sense to have an extra cache for instructions. This is accomplished
by having instruction only caches at L1 level, called L1-icache. Normally, as code size is dependent upon the program complexity,
which is fixed, this means that the growth is not as bad as in the case of data. Additionally, program flow being more predictable
than data access patterns, it is normally the case that active code can somewhat fit in the cpu caches.
However, it is still not uncommon to find applications using big shared libraries. In cases when the size of shared library exceeds
the cache-sizes or for some reason prefetching fails, the instructions will need to be fetched from the main memory.
This can result in two problems, increased latencies due to instructions being fetched from remote node and increased overhead on
memory controllers and interconnects due to congestion. In this work, we examine and analyze the cost of code on a remote NUMA
node from both these perspectives and discuss the results. We also discuss some ideas that can be applied to mitigate the problem.

The remainder of the paper is organized as follows. In the next section, we discuss a little bit about shared libraries and 
then talk about our experimental setup. In section \ref{sec:analysis}, we will discuss at length the motivation, results and also the explanation
of the results for each of the experiments performed. In section \ref{sec:future-work}, we will discuss some ideas that could potentially improve
the system performance with shared libraries on NUMA machines and talk about the roadmap to their implementation.
Finally we conclude, highlighting the key observations from our experiments.

